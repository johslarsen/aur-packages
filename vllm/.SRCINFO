pkgbase = vllm
	pkgdesc = A high-throughput and memory-efficient inference and serving engine for LLMs
	pkgver = 0.4.3
	pkgrel = 1
	url = https://github.com/vllm-project/vllm
	arch = x86_64
	license = APACHE
	makedepends = cmake
	makedepends = ninja
	makedepends = python-setuptools
	makedepends = cuda
	makedepends = cudnn
	makedepends = nccl
	makedepends = magma-cuda
	makedepends = rocm-hip-sdk>=6.0.0
	makedepends = miopen-hip
	makedepends = rccl
	makedepends = magma-hip
	depends = python-pytorch>=2.3.0
	provides = vllm
	conflicts = vllm
	source = vllm-0.4.3.tar.gz::https://github.com/vllm-project/vllm/archive/refs/tags/v0.4.3.tar.gz
	sha256sums = 37eb50327aa72444bb0901463c0c58f141fd1cc462f7ebe76ab23dea61d4cefe

pkgname = vllm

pkgname = vllm-cuda
	depends = python-pytorch>=2.3.0
	depends = python-pytorch-cuda>=2.3.0

pkgname = vllm-rocm
	depends = python-pytorch>=2.3.0
	depends = python-pytorch-rocm>=2.3.0
